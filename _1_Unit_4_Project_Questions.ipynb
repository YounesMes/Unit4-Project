{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YounesMes/Unit4-Project/blob/master/_1_Unit_4_Project_Questions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pcBqRLqzMULW"
      },
      "source": [
        "# Unit 4 Project\n",
        "After understanding how neural networks work, implementing some basic architectures using deep learning frameworks, and learning about some advanced techniques to help enhance our neural networks' models results, it's time to apply what you learned! So let's start\n",
        "\n",
        "## Project Overview \n",
        "In this project, you will build a neural network model to classify images from CIFAR 10 dataset. \n",
        "\n",
        "The CIFAR-10 dataset consists of 60000 32x32 color images of 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images. The dataset is divided into five training batches and one test batch, each with 10000 images. The test batch contains exactly 1000 randomly-selected images from each class. The training batches contain the remaining images in random order, but some training batches may contain more images from one class than another. Between them, the training batches contain exactly 5000 images from each class. [source](https://www.cs.toronto.edu/~kriz/cifar.html)\n",
        "\n",
        "Unlike the previous projects, there will be no code cells to fill, the only task you have is to build the best possible model using the techniques you learned about in this unit. But we will guide you with some directives.\n",
        "\n",
        "You will have enough guidance throughout the project and your work will be reviewed and graded by a teacher assistant. You can also reach out to the TA via slack whenever you feel you are stuck.\n",
        "\n",
        "## Some guidelines\n",
        "- Please use text cells to write the questions' answers in a good way.\n",
        "- Don't forget to save the different models you tested so you will be able to report the different results you got and the impact of the different techniques you tested later.\n",
        "\n",
        "## Getting started\n",
        "- In case you don't have a GPU, it is recommended that you use google colab. Start by cloning this repository, then open [google colab](https://colab.research.google.com/), click on File > Upload notebook, and finally upload the ```.ipynb``` file from the repository you have just cloned! Don't forget to change the runtime to GPU. If you want to work in your local environment just open it using jupyter notebook.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kLavLevW45rD"
      },
      "source": [
        "## Dataset \n",
        "\n",
        "1.   Load the dataset (**hint**: it's available here https://keras.io/api/datasets/)\n",
        "2.   Display few images of each class.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Q7D9q0qVMj98"
      },
      "outputs": [],
      "source": [
        "#importing librairies\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from PIL import Image\n",
        "%matplotlib inline\n",
        "from matplotlib.pyplot import imshow\n",
        "from tensorflow.keras import datasets, layers, Input,models\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Flatten, Dense, Dropout\n",
        "from keras.layers import BatchNormalization\n",
        "from keras import regularizers\n",
        "from tensorflow import keras\n",
        "import os\n",
        "import pickle as pk\n",
        "import seaborn as sn\n",
        "import io\n",
        "import datetime\n",
        "from tensorflow.keras.losses import BinaryCrossentropy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "PjCuHlIcdsp5",
        "outputId": "8cde8514-593b-45d4-f0aa-f7e2a588427c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 3s 0us/step\n",
            "170508288/170498071 [==============================] - 3s 0us/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(50000, 32, 32, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "#importing the dataset\n",
        "import ssl\n",
        "ssl._create_default_https_context=ssl._create_unverified_context\n",
        "(X_train, y_train), (X_test,y_test) = datasets.cifar10.load_data()\n",
        "X_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "0v1jWbQNdsp7"
      },
      "outputs": [],
      "source": [
        "y_train = y_train.reshape(-1,)\n",
        "y_train[:5]\n",
        "\n",
        "#define the classes:\n",
        "classes = [\"airplane\",\"automobile\",\"bird\",\"cat\",\"deer\",\"dog\",\"frog\",\"horse\",\"ship\",\"truck\"]\n",
        "\n",
        "def plot_simp(x,y,indx):\n",
        "    plt.figure(figsize=(10,3))\n",
        "    plt.imshow(x[indx])\n",
        "    plt.xlabel(classes[y[indx]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Ol2HOVFgdsp9",
        "outputId": "be972110-f5ba-4e98-e377-dcf4a92e4280",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x216 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "plot_simp(X_train,y_train,3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YmIZ1vTp6pDZ"
      },
      "source": [
        "## Baseline Model\n",
        "\n",
        "1.   Build a basline model using only dense layers, activation function of your choice, and the adapted cost function for this problem.\n",
        "2.   Train and evaluate your model\n",
        "3.   Analyze the result\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dj0D8kHF9_K3"
      },
      "outputs": [],
      "source": [
        "# Your code here\n",
        "X_train = X_train / 255.0\n",
        "X_test = X_test / 255.0\n",
        "model= models.Sequential([\n",
        "        layers.Flatten(input_shape=(32,32,3)),\n",
        "        layers.Dense(3000, activation='relu'),\n",
        "        layers.Dense(1000, activation='relu'),\n",
        "        layers.Dense(10, activation='sigmoid')    \n",
        "    ])\n",
        "\n",
        "model.compile(optimizer='SGD',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(X_train, y_train, epochs=50)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(X_test,y_test)"
      ],
      "metadata": {
        "id": "iAdou5nUjJp6",
        "outputId": "84c54d35-953b-41ba-9525-a437ad00cce0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 2s 5ms/step - loss: 1.5792 - accuracy: 0.5637\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.57919442653656, 0.5637000203132629]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**We got a precision of 91% in the training with 50 epochs, and 56% in the test samples. the neural network is performing bad in this task**"
      ],
      "metadata": {
        "id": "YqxyWWkKktdX"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GvygWbNy-BxD"
      },
      "source": [
        "## Accelerating the training\n",
        "\n",
        "1.   Add batch normalization layers to your network in order to accelerate the training. Start with adding batch norm layer before each of your activation layers.\n",
        "2.   Analyze the new results\n",
        "3.   Change the position of the batch norm layers so they will be after the activation layers. Compare the results.\n",
        "   \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fNaAVBN4-Ai4",
        "outputId": "0fb9222c-5c31-45e8-a8eb-9322f07b8ec7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "1563/1563 [==============================] - 15s 9ms/step - loss: 1.7055 - accuracy: 0.4181\n",
            "Epoch 2/50\n",
            "1563/1563 [==============================] - 13s 9ms/step - loss: 1.3737 - accuracy: 0.5167\n",
            "Epoch 3/50\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 1.2358 - accuracy: 0.5662\n",
            "Epoch 4/50\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 1.1263 - accuracy: 0.6047\n",
            "Epoch 5/50\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 1.0332 - accuracy: 0.6374\n",
            "Epoch 6/50\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 0.9479 - accuracy: 0.6721\n",
            "Epoch 7/50\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 0.8633 - accuracy: 0.7013\n",
            "Epoch 8/50\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 0.7900 - accuracy: 0.7288\n",
            "Epoch 9/50\n",
            "1563/1563 [==============================] - 14s 9ms/step - loss: 0.7264 - accuracy: 0.7473\n",
            "Epoch 10/50\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 0.6573 - accuracy: 0.7741\n",
            "Epoch 11/50\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 0.6022 - accuracy: 0.7947\n",
            "Epoch 12/50\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 0.5590 - accuracy: 0.8082\n",
            "Epoch 13/50\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 0.5000 - accuracy: 0.8290\n",
            "Epoch 14/50\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 0.4646 - accuracy: 0.8413\n",
            "Epoch 15/50\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 0.4300 - accuracy: 0.8544\n",
            "Epoch 16/50\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 0.3926 - accuracy: 0.8673\n",
            "Epoch 17/50\n",
            "1563/1563 [==============================] - 13s 9ms/step - loss: 0.3652 - accuracy: 0.8757\n",
            "Epoch 18/50\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 0.3325 - accuracy: 0.8892\n",
            "Epoch 19/50\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 0.3079 - accuracy: 0.8965\n",
            "Epoch 20/50\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 0.2933 - accuracy: 0.9015\n",
            "Epoch 21/50\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 0.2568 - accuracy: 0.9153\n",
            "Epoch 22/50\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 0.2403 - accuracy: 0.9213\n",
            "Epoch 23/50\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 0.2276 - accuracy: 0.9241\n",
            "Epoch 24/50\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 0.2185 - accuracy: 0.9262\n",
            "Epoch 25/50\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 0.2031 - accuracy: 0.9331\n",
            "Epoch 26/50\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 0.1890 - accuracy: 0.9376\n",
            "Epoch 27/50\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 0.1798 - accuracy: 0.9403\n",
            "Epoch 28/50\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 0.1674 - accuracy: 0.9449\n",
            "Epoch 29/50\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 0.1575 - accuracy: 0.9495\n",
            "Epoch 30/50\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 0.1555 - accuracy: 0.9502\n",
            "Epoch 31/50\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 0.1442 - accuracy: 0.9530\n",
            "Epoch 32/50\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 0.1321 - accuracy: 0.9564\n",
            "Epoch 33/50\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 0.1265 - accuracy: 0.9591\n",
            "Epoch 34/50\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 0.1208 - accuracy: 0.9607\n",
            "Epoch 35/50\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 0.1171 - accuracy: 0.9620\n",
            "Epoch 36/50\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 0.1102 - accuracy: 0.9655\n",
            "Epoch 37/50\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 0.1088 - accuracy: 0.9640\n",
            "Epoch 38/50\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 0.1003 - accuracy: 0.9675\n",
            "Epoch 39/50\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 0.0976 - accuracy: 0.9683\n",
            "Epoch 40/50\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 0.0902 - accuracy: 0.9714\n",
            "Epoch 41/50\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 0.0967 - accuracy: 0.9688\n",
            "Epoch 42/50\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 0.0939 - accuracy: 0.9695\n",
            "Epoch 43/50\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 0.0844 - accuracy: 0.9720\n",
            "Epoch 44/50\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 0.0819 - accuracy: 0.9740\n",
            "Epoch 45/50\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 0.0721 - accuracy: 0.9772\n",
            "Epoch 46/50\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 0.0709 - accuracy: 0.9776\n",
            "Epoch 47/50\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 0.0693 - accuracy: 0.9785\n",
            "Epoch 48/50\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 0.0687 - accuracy: 0.9785\n",
            "Epoch 49/50\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 0.0641 - accuracy: 0.9800\n",
            "Epoch 50/50\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 0.0650 - accuracy: 0.9790\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fc9f01f1090>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "# before the activation layers\n",
        "model_2= models.Sequential([\n",
        "        layers.Flatten(input_shape=(32,32,3)),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        layers.Dense(3000, activation='relu'),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        layers.Dense(1000, activation='relu'),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        layers.Dense(10, activation='sigmoid')    \n",
        "    ])\n",
        "\n",
        "model_2.compile(optimizer='SGD',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model_2.fit(X_train, y_train, epochs=50)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "GILVsCSONtu3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_2.evaluate(X_test,y_test)"
      ],
      "metadata": {
        "id": "WfB2BWv09fmA",
        "outputId": "0cfe1821-3288-43ad-8a3c-b30c71fb823c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 2s 5ms/step - loss: 2.2586 - accuracy: 0.5603\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2.2585885524749756, 0.5602999925613403]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = X_train / 255.0\n",
        "X_test = X_test / 255.0"
      ],
      "metadata": {
        "id": "tAy8LtAPRj-7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# after the activation layers\n",
        "model_3= models.Sequential([\n",
        "        layers.Flatten(input_shape=(32,32,3)),\n",
        "        layers.Dense(3000, activation='relu'),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        layers.Dense(1000, activation='relu'),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        layers.Dense(10, activation='sigmoid'),\n",
        "        #tf.keras.layers.BatchNormalization()   \n",
        "    ])\n",
        "\n",
        "model_3.compile(optimizer='SGD',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model_3.fit(X_train, y_train, epochs=50)"
      ],
      "metadata": {
        "id": "MZ4JMafTMCSV",
        "outputId": "55faddee-b956-452e-aa84-73d66983e987",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "1563/1563 [==============================] - 14s 8ms/step - loss: 1.7614 - accuracy: 0.3987\n",
            "Epoch 2/50\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 1.5016 - accuracy: 0.4728\n",
            "Epoch 3/50\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 1.4213 - accuracy: 0.5022\n",
            "Epoch 4/50\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 1.3758 - accuracy: 0.5176\n",
            "Epoch 5/50\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 1.3429 - accuracy: 0.5273\n",
            "Epoch 6/50\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 1.3425 - accuracy: 0.5278\n",
            "Epoch 7/50\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 1.3436 - accuracy: 0.5250\n",
            "Epoch 8/50\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 1.3419 - accuracy: 0.5259\n",
            "Epoch 9/50\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 1.3456 - accuracy: 0.5289\n",
            "Epoch 10/50\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 1.3369 - accuracy: 0.5303\n",
            "Epoch 11/50\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 1.3363 - accuracy: 0.5264\n",
            "Epoch 12/50\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 1.3253 - accuracy: 0.5299\n",
            "Epoch 13/50\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 1.3235 - accuracy: 0.5320\n",
            "Epoch 14/50\n",
            "1563/1563 [==============================] - 14s 9ms/step - loss: 1.3250 - accuracy: 0.5332\n",
            "Epoch 15/50\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 1.3270 - accuracy: 0.5276\n",
            "Epoch 16/50\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 1.3263 - accuracy: 0.5296\n",
            "Epoch 17/50\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 1.3297 - accuracy: 0.5328\n",
            "Epoch 18/50\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 1.3215 - accuracy: 0.5333\n",
            "Epoch 19/50\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 1.3145 - accuracy: 0.5364\n",
            "Epoch 20/50\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 1.3066 - accuracy: 0.5352\n",
            "Epoch 21/50\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 1.3151 - accuracy: 0.5355\n",
            "Epoch 22/50\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 1.3004 - accuracy: 0.5387\n",
            "Epoch 23/50\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 1.2903 - accuracy: 0.5414\n",
            "Epoch 24/50\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 1.2601 - accuracy: 0.5531\n",
            "Epoch 25/50\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 1.2642 - accuracy: 0.5537\n",
            "Epoch 26/50\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 1.2764 - accuracy: 0.5507\n",
            "Epoch 27/50\n",
            "1563/1563 [==============================] - 12s 8ms/step - loss: 1.2809 - accuracy: 0.5456\n",
            "Epoch 28/50\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 1.2558 - accuracy: 0.5542\n",
            "Epoch 29/50\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 1.2398 - accuracy: 0.5601\n",
            "Epoch 30/50\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 1.2469 - accuracy: 0.5585\n",
            "Epoch 31/50\n",
            "1563/1563 [==============================] - 12s 8ms/step - loss: 1.2601 - accuracy: 0.5511\n",
            "Epoch 32/50\n",
            "1563/1563 [==============================] - 12s 8ms/step - loss: 1.2628 - accuracy: 0.5532\n",
            "Epoch 33/50\n",
            "1563/1563 [==============================] - 14s 9ms/step - loss: 1.2411 - accuracy: 0.5589\n",
            "Epoch 34/50\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 1.2083 - accuracy: 0.5732\n",
            "Epoch 35/50\n",
            "1563/1563 [==============================] - 12s 8ms/step - loss: 1.2067 - accuracy: 0.5709\n",
            "Epoch 36/50\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 1.2017 - accuracy: 0.5750\n",
            "Epoch 37/50\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 1.1797 - accuracy: 0.5802\n",
            "Epoch 38/50\n",
            "1563/1563 [==============================] - 12s 8ms/step - loss: 1.1562 - accuracy: 0.5904\n",
            "Epoch 39/50\n",
            "1563/1563 [==============================] - 12s 8ms/step - loss: 1.1477 - accuracy: 0.5911\n",
            "Epoch 40/50\n",
            "1563/1563 [==============================] - 12s 8ms/step - loss: 1.1460 - accuracy: 0.5943\n",
            "Epoch 41/50\n",
            "1563/1563 [==============================] - 12s 8ms/step - loss: 1.1321 - accuracy: 0.5981\n",
            "Epoch 42/50\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 1.1152 - accuracy: 0.6051\n",
            "Epoch 43/50\n",
            "1563/1563 [==============================] - 12s 8ms/step - loss: 1.0916 - accuracy: 0.6127\n",
            "Epoch 44/50\n",
            "1563/1563 [==============================] - 12s 8ms/step - loss: 1.0837 - accuracy: 0.6148\n",
            "Epoch 45/50\n",
            "1563/1563 [==============================] - 12s 8ms/step - loss: 1.0794 - accuracy: 0.6170\n",
            "Epoch 46/50\n",
            "1563/1563 [==============================] - 12s 8ms/step - loss: 1.0621 - accuracy: 0.6251\n",
            "Epoch 47/50\n",
            "1563/1563 [==============================] - 12s 8ms/step - loss: 1.0779 - accuracy: 0.6164\n",
            "Epoch 48/50\n",
            "1563/1563 [==============================] - 12s 8ms/step - loss: 1.0723 - accuracy: 0.6195\n",
            "Epoch 49/50\n",
            "1563/1563 [==============================] - 12s 8ms/step - loss: 1.0665 - accuracy: 0.6195\n",
            "Epoch 50/50\n",
            "1563/1563 [==============================] - 12s 8ms/step - loss: 1.0654 - accuracy: 0.6217\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fc97bb31e90>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_3.evaluate(X_test,y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CKRBtuw0Rnt3",
        "outputId": "5dd0ebce-82ea-48e5-fd11-d3c56cd860a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 2s 5ms/step - loss: 1.8123 - accuracy: 0.4647\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.8123236894607544, 0.46470001339912415]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We got a better result when adding the batch normlization layer \"Before the activation\"\n",
        "- Precision in training : 97%\n",
        "- Precision in the test : 56%"
      ],
      "metadata": {
        "id": "EoL9wadFTOxD"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XYXf6uM6DJ1a"
      },
      "source": [
        "## Reducing the overfitting \n",
        "\n",
        "\n",
        "1.   Apply the dropout technique to reduce the overfitting your model is suffering from\n",
        "2.   Try different dropout rates \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "2er70cnXDIqI",
        "outputId": "4d9b6d9e-781f-4a54-c247-c5518ff3d33f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "1563/1563 [==============================] - 15s 9ms/step - loss: 2.0263 - accuracy: 0.3161\n",
            "Epoch 2/50\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 1.7385 - accuracy: 0.3809\n",
            "Epoch 3/50\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 1.6597 - accuracy: 0.4094\n",
            "Epoch 4/50\n",
            "1563/1563 [==============================] - 13s 9ms/step - loss: 1.6215 - accuracy: 0.4217\n",
            "Epoch 5/50\n",
            "1563/1563 [==============================] - 13s 9ms/step - loss: 1.5824 - accuracy: 0.4369\n",
            "Epoch 6/50\n",
            "1563/1563 [==============================] - 14s 9ms/step - loss: 1.5604 - accuracy: 0.4438\n",
            "Epoch 7/50\n",
            "1563/1563 [==============================] - 14s 9ms/step - loss: 1.5333 - accuracy: 0.4541\n",
            "Epoch 8/50\n",
            "1563/1563 [==============================] - 13s 9ms/step - loss: 1.5170 - accuracy: 0.4612\n",
            "Epoch 9/50\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 1.4981 - accuracy: 0.4660\n",
            "Epoch 10/50\n",
            "1563/1563 [==============================] - 13s 9ms/step - loss: 1.4768 - accuracy: 0.4722\n",
            "Epoch 11/50\n",
            "1563/1563 [==============================] - 13s 9ms/step - loss: 1.4599 - accuracy: 0.4793\n",
            "Epoch 12/50\n",
            "1563/1563 [==============================] - 13s 9ms/step - loss: 1.4424 - accuracy: 0.4859\n",
            "Epoch 13/50\n",
            "1563/1563 [==============================] - 14s 9ms/step - loss: 1.4308 - accuracy: 0.4882\n",
            "Epoch 14/50\n",
            "1563/1563 [==============================] - 14s 9ms/step - loss: 1.4089 - accuracy: 0.4966\n",
            "Epoch 15/50\n",
            "1563/1563 [==============================] - 13s 9ms/step - loss: 1.4025 - accuracy: 0.4996\n",
            "Epoch 16/50\n",
            "1563/1563 [==============================] - 13s 9ms/step - loss: 1.3853 - accuracy: 0.5066\n",
            "Epoch 17/50\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 1.3805 - accuracy: 0.5071\n",
            "Epoch 18/50\n",
            "1563/1563 [==============================] - 13s 9ms/step - loss: 1.3654 - accuracy: 0.5107\n",
            "Epoch 19/50\n",
            "1563/1563 [==============================] - 13s 9ms/step - loss: 1.3491 - accuracy: 0.5190\n",
            "Epoch 20/50\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 1.3419 - accuracy: 0.5210\n",
            "Epoch 21/50\n",
            "1563/1563 [==============================] - 13s 9ms/step - loss: 1.3331 - accuracy: 0.5252\n",
            "Epoch 22/50\n",
            "1563/1563 [==============================] - 13s 9ms/step - loss: 1.3191 - accuracy: 0.5302\n",
            "Epoch 23/50\n",
            "1563/1563 [==============================] - 13s 9ms/step - loss: 1.3095 - accuracy: 0.5324\n",
            "Epoch 24/50\n",
            "1563/1563 [==============================] - 13s 9ms/step - loss: 1.3021 - accuracy: 0.5356\n",
            "Epoch 25/50\n",
            "1563/1563 [==============================] - 13s 9ms/step - loss: 1.2943 - accuracy: 0.5381\n",
            "Epoch 26/50\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 1.2813 - accuracy: 0.5428\n",
            "Epoch 27/50\n",
            "1563/1563 [==============================] - 13s 9ms/step - loss: 1.2715 - accuracy: 0.5463\n",
            "Epoch 28/50\n",
            "1563/1563 [==============================] - 14s 9ms/step - loss: 1.2603 - accuracy: 0.5500\n",
            "Epoch 29/50\n",
            "1563/1563 [==============================] - 14s 9ms/step - loss: 1.2514 - accuracy: 0.5546\n",
            "Epoch 30/50\n",
            "1563/1563 [==============================] - 14s 9ms/step - loss: 1.2496 - accuracy: 0.5545\n",
            "Epoch 31/50\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 1.2394 - accuracy: 0.5565\n",
            "Epoch 32/50\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 1.2300 - accuracy: 0.5603\n",
            "Epoch 33/50\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 1.2184 - accuracy: 0.5638\n",
            "Epoch 34/50\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 1.2136 - accuracy: 0.5673\n",
            "Epoch 35/50\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 1.2019 - accuracy: 0.5681\n",
            "Epoch 36/50\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 1.1929 - accuracy: 0.5744\n",
            "Epoch 37/50\n",
            "1563/1563 [==============================] - 13s 9ms/step - loss: 1.1851 - accuracy: 0.5767\n",
            "Epoch 38/50\n",
            "1563/1563 [==============================] - 14s 9ms/step - loss: 1.1799 - accuracy: 0.5808\n",
            "Epoch 39/50\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 1.1736 - accuracy: 0.5818\n",
            "Epoch 40/50\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 1.1661 - accuracy: 0.5838\n",
            "Epoch 41/50\n",
            "1563/1563 [==============================] - 13s 9ms/step - loss: 1.1530 - accuracy: 0.5905\n",
            "Epoch 42/50\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 1.1467 - accuracy: 0.5920\n",
            "Epoch 43/50\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 1.1399 - accuracy: 0.5932\n",
            "Epoch 44/50\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 1.1351 - accuracy: 0.5946\n",
            "Epoch 45/50\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 1.1279 - accuracy: 0.5958\n",
            "Epoch 46/50\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 1.1172 - accuracy: 0.6012\n",
            "Epoch 47/50\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 1.1171 - accuracy: 0.5989\n",
            "Epoch 48/50\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 1.1040 - accuracy: 0.6070\n",
            "Epoch 49/50\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 1.0992 - accuracy: 0.6046\n",
            "Epoch 50/50\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 1.0860 - accuracy: 0.6125\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7ff5c8ecb7d0>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "# adding Dropout layers\n",
        "model_2_= models.Sequential([\n",
        "        layers.Flatten(input_shape=(32,32,3)),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        layers.Dense(3000, activation='relu'),\n",
        "        tf.keras.layers.Dropout(0.5),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        layers.Dense(1000, activation='relu'),\n",
        "        tf.keras.layers.Dropout(0.5),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        layers.Dense(10, activation='sigmoid'),\n",
        "        #tf.keras.layers.Dropout(0.5)  \n",
        "    ])\n",
        "\n",
        "model_2_.compile(optimizer='SGD',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model_2_.fit(X_train, y_train, epochs=50)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_2_.evaluate(X_test,y_test)"
      ],
      "metadata": {
        "id": "Pg45Df_dbLnH",
        "outputId": "6f1fa345-0156-4545-b298-2ea1f9279104",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 2s 5ms/step - loss: 1.1956 - accuracy: 0.5773\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.1956161260604858, 0.5773000121116638]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The accuracy result for the training is near from the one of testing so aour model is not overfiting \n",
        "but the NN still performing bad on this task"
      ],
      "metadata": {
        "id": "_aVVg18-W2TS"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cZkBwzPX_eHh"
      },
      "source": [
        "## Trying different model's parameters\n",
        "1. Try changing the number of layers, the number of hidden neurons in each layer, the activation functions, the weight initialization method...\n",
        "2. Compare the results you got for each evaluated model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sXtHgGpjFm9i"
      },
      "outputs": [],
      "source": [
        "# Your code here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model 1 : "
      ],
      "metadata": {
        "id": "leA8abaeYDQW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_nn_1= models.Sequential([\n",
        "        layers.Flatten(input_shape=(32,32,3)),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        layers.Dense(3000, activation=keras.layers.LeakyReLU(alpha=0.01)),\n",
        "        tf.keras.layers.Dropout(0.5),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        layers.Dense(1000, activation=keras.layers.LeakyReLU(alpha=0.01)),\n",
        "        tf.keras.layers.Dropout(0.5),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        layers.Dense(10, activation='sigmoid'),\n",
        "        #tf.keras.layers.Dropout(0.5)  \n",
        "    ])\n",
        "\n",
        "model_nn_1.compile(optimizer=tf.keras.optimizers.Adam(\n",
        "     learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-07, amsgrad=False,\n",
        "     name='Adam'),\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model_nn_1.fit(X_train, y_train, epochs=50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4xlDqmeLYNHJ",
        "outputId": "1c0bfda0-b6ca-42d2-a195-23cdf4ab0241"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "1563/1563 [==============================] - 17s 10ms/step - loss: 1.9488 - accuracy: 0.3373\n",
            "Epoch 2/50\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 1.6887 - accuracy: 0.3996\n",
            "Epoch 3/50\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 1.6175 - accuracy: 0.4233\n",
            "Epoch 4/50\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 1.5661 - accuracy: 0.4439\n",
            "Epoch 5/50\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 1.5278 - accuracy: 0.4554\n",
            "Epoch 6/50\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 1.5032 - accuracy: 0.4635\n",
            "Epoch 7/50\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 1.4727 - accuracy: 0.4774\n",
            "Epoch 8/50\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 1.4490 - accuracy: 0.4828\n",
            "Epoch 9/50\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 1.4283 - accuracy: 0.4930\n",
            "Epoch 10/50\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 1.4131 - accuracy: 0.4962\n",
            "Epoch 11/50\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 1.3890 - accuracy: 0.5050\n",
            "Epoch 12/50\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 1.3747 - accuracy: 0.5129\n",
            "Epoch 13/50\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 1.3596 - accuracy: 0.5162\n",
            "Epoch 14/50\n",
            "1563/1563 [==============================] - 18s 11ms/step - loss: 1.3427 - accuracy: 0.5217\n",
            "Epoch 15/50\n",
            "1563/1563 [==============================] - 19s 12ms/step - loss: 1.3299 - accuracy: 0.5256\n",
            "Epoch 16/50\n",
            "1563/1563 [==============================] - 19s 12ms/step - loss: 1.3210 - accuracy: 0.5306\n",
            "Epoch 17/50\n",
            "1563/1563 [==============================] - 17s 11ms/step - loss: 1.3074 - accuracy: 0.5344\n",
            "Epoch 18/50\n",
            "1563/1563 [==============================] - 17s 11ms/step - loss: 1.2864 - accuracy: 0.5414\n",
            "Epoch 19/50\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 1.2866 - accuracy: 0.5409\n",
            "Epoch 20/50\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 1.2775 - accuracy: 0.5423\n",
            "Epoch 21/50\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 1.2675 - accuracy: 0.5479\n",
            "Epoch 22/50\n",
            "1563/1563 [==============================] - 17s 11ms/step - loss: 1.2532 - accuracy: 0.5538\n",
            "Epoch 23/50\n",
            "1563/1563 [==============================] - 17s 11ms/step - loss: 1.2537 - accuracy: 0.5534\n",
            "Epoch 24/50\n",
            "1563/1563 [==============================] - 17s 11ms/step - loss: 1.2416 - accuracy: 0.5566\n",
            "Epoch 25/50\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 1.2339 - accuracy: 0.5590\n",
            "Epoch 26/50\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 1.2173 - accuracy: 0.5673\n",
            "Epoch 27/50\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 1.2120 - accuracy: 0.5700\n",
            "Epoch 28/50\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 1.2108 - accuracy: 0.5715\n",
            "Epoch 29/50\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 1.2032 - accuracy: 0.5738\n",
            "Epoch 30/50\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 1.1928 - accuracy: 0.5776\n",
            "Epoch 31/50\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 1.1854 - accuracy: 0.5772\n",
            "Epoch 32/50\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 1.1763 - accuracy: 0.5821\n",
            "Epoch 33/50\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 1.1716 - accuracy: 0.5808\n",
            "Epoch 34/50\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 1.1640 - accuracy: 0.5861\n",
            "Epoch 35/50\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 1.1563 - accuracy: 0.5891\n",
            "Epoch 36/50\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 1.1529 - accuracy: 0.5876\n",
            "Epoch 37/50\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 1.1468 - accuracy: 0.5918\n",
            "Epoch 38/50\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 1.1400 - accuracy: 0.5938\n",
            "Epoch 39/50\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 1.1401 - accuracy: 0.5914\n",
            "Epoch 40/50\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 1.1329 - accuracy: 0.5944\n",
            "Epoch 41/50\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 1.1209 - accuracy: 0.6008\n",
            "Epoch 42/50\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 1.1176 - accuracy: 0.6019\n",
            "Epoch 43/50\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 1.1134 - accuracy: 0.6029\n",
            "Epoch 44/50\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 1.1097 - accuracy: 0.6061\n",
            "Epoch 45/50\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 1.1044 - accuracy: 0.6057\n",
            "Epoch 46/50\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 1.0979 - accuracy: 0.6079\n",
            "Epoch 47/50\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 1.0866 - accuracy: 0.6133\n",
            "Epoch 48/50\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 1.0882 - accuracy: 0.6118\n",
            "Epoch 49/50\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 1.0794 - accuracy: 0.6146\n",
            "Epoch 50/50\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 1.0767 - accuracy: 0.6155\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fd7869374d0>"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_nn_2= models.Sequential([\n",
        "        layers.Flatten(input_shape=(32,32,3)),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        layers.Dense(5000, activation='relu'),\n",
        "        tf.keras.layers.Dropout(0.5),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        layers.Dense(2000, activation='relu'),\n",
        "        tf.keras.layers.Dropout(0.5),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        layers.Dense(10, activation='sigmoid'),\n",
        "        #tf.keras.layers.Dropout(0.5)  \n",
        "    ])\n",
        "\n",
        "model_nn_2.compile(optimizer='SGD',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model_nn_2.fit(X_train, y_train, epochs=50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8jsJErlPjOi_",
        "outputId": "e07e3d84-f7db-48a6-8da4-be4e0e38eaf7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "1563/1563 [==============================] - 21s 13ms/step - loss: 2.0308 - accuracy: 0.3260\n",
            "Epoch 2/50\n",
            "1563/1563 [==============================] - 20s 13ms/step - loss: 1.7660 - accuracy: 0.3846\n",
            "Epoch 3/50\n",
            "1563/1563 [==============================] - 20s 12ms/step - loss: 1.6812 - accuracy: 0.4085\n",
            "Epoch 4/50\n",
            "1563/1563 [==============================] - 19s 12ms/step - loss: 1.6374 - accuracy: 0.4220\n",
            "Epoch 5/50\n",
            "1563/1563 [==============================] - 20s 13ms/step - loss: 1.6101 - accuracy: 0.4313\n",
            "Epoch 6/50\n",
            "1563/1563 [==============================] - 20s 13ms/step - loss: 1.5753 - accuracy: 0.4446\n",
            "Epoch 7/50\n",
            "1563/1563 [==============================] - 19s 12ms/step - loss: 1.5492 - accuracy: 0.4538\n",
            "Epoch 8/50\n",
            "1563/1563 [==============================] - 19s 12ms/step - loss: 1.5291 - accuracy: 0.4606\n",
            "Epoch 9/50\n",
            "1563/1563 [==============================] - 20s 12ms/step - loss: 1.5091 - accuracy: 0.4672\n",
            "Epoch 10/50\n",
            "1563/1563 [==============================] - 19s 12ms/step - loss: 1.4939 - accuracy: 0.4713\n",
            "Epoch 11/50\n",
            "1563/1563 [==============================] - 20s 13ms/step - loss: 1.4747 - accuracy: 0.4772\n",
            "Epoch 12/50\n",
            "1563/1563 [==============================] - 20s 13ms/step - loss: 1.4517 - accuracy: 0.4867\n",
            "Epoch 13/50\n",
            "1563/1563 [==============================] - 20s 13ms/step - loss: 1.4322 - accuracy: 0.4907\n",
            "Epoch 14/50\n",
            "1563/1563 [==============================] - 20s 13ms/step - loss: 1.4261 - accuracy: 0.4971\n",
            "Epoch 15/50\n",
            "1563/1563 [==============================] - 20s 13ms/step - loss: 1.4068 - accuracy: 0.5025\n",
            "Epoch 16/50\n",
            "1563/1563 [==============================] - 20s 13ms/step - loss: 1.3911 - accuracy: 0.5067\n",
            "Epoch 17/50\n",
            "1563/1563 [==============================] - 20s 13ms/step - loss: 1.3775 - accuracy: 0.5106\n",
            "Epoch 18/50\n",
            "1563/1563 [==============================] - 20s 13ms/step - loss: 1.3745 - accuracy: 0.5141\n",
            "Epoch 19/50\n",
            "1563/1563 [==============================] - 20s 13ms/step - loss: 1.3521 - accuracy: 0.5199\n",
            "Epoch 20/50\n",
            "1563/1563 [==============================] - 20s 13ms/step - loss: 1.3426 - accuracy: 0.5256\n",
            "Epoch 21/50\n",
            "1563/1563 [==============================] - 20s 13ms/step - loss: 1.3322 - accuracy: 0.5292\n",
            "Epoch 22/50\n",
            "1563/1563 [==============================] - 20s 13ms/step - loss: 1.3243 - accuracy: 0.5314\n",
            "Epoch 23/50\n",
            "1563/1563 [==============================] - 20s 13ms/step - loss: 1.3073 - accuracy: 0.5385\n",
            "Epoch 24/50\n",
            "1563/1563 [==============================] - 20s 13ms/step - loss: 1.3012 - accuracy: 0.5396\n",
            "Epoch 25/50\n",
            "1563/1563 [==============================] - 20s 13ms/step - loss: 1.2835 - accuracy: 0.5455\n",
            "Epoch 26/50\n",
            "1563/1563 [==============================] - 20s 13ms/step - loss: 1.2763 - accuracy: 0.5473\n",
            "Epoch 27/50\n",
            "1563/1563 [==============================] - 20s 13ms/step - loss: 1.2678 - accuracy: 0.5498\n",
            "Epoch 28/50\n",
            "1563/1563 [==============================] - 20s 13ms/step - loss: 1.2546 - accuracy: 0.5555\n",
            "Epoch 29/50\n",
            "1563/1563 [==============================] - 20s 13ms/step - loss: 1.2433 - accuracy: 0.5600\n",
            "Epoch 30/50\n",
            "1563/1563 [==============================] - 20s 13ms/step - loss: 1.2304 - accuracy: 0.5648\n",
            "Epoch 31/50\n",
            "1563/1563 [==============================] - 20s 13ms/step - loss: 1.2233 - accuracy: 0.5672\n",
            "Epoch 32/50\n",
            "1563/1563 [==============================] - 20s 13ms/step - loss: 1.2148 - accuracy: 0.5679\n",
            "Epoch 33/50\n",
            "1563/1563 [==============================] - 20s 13ms/step - loss: 1.2073 - accuracy: 0.5715\n",
            "Epoch 34/50\n",
            "1563/1563 [==============================] - 20s 13ms/step - loss: 1.1972 - accuracy: 0.5758\n",
            "Epoch 35/50\n",
            "1563/1563 [==============================] - 20s 13ms/step - loss: 1.1836 - accuracy: 0.5813\n",
            "Epoch 36/50\n",
            "1563/1563 [==============================] - 20s 13ms/step - loss: 1.1794 - accuracy: 0.5791\n",
            "Epoch 37/50\n",
            "1563/1563 [==============================] - 20s 13ms/step - loss: 1.1686 - accuracy: 0.5881\n",
            "Epoch 38/50\n",
            "1563/1563 [==============================] - 20s 13ms/step - loss: 1.1610 - accuracy: 0.5910\n",
            "Epoch 39/50\n",
            "1563/1563 [==============================] - 20s 13ms/step - loss: 1.1597 - accuracy: 0.5888\n",
            "Epoch 40/50\n",
            "1563/1563 [==============================] - 20s 13ms/step - loss: 1.1407 - accuracy: 0.5951\n",
            "Epoch 41/50\n",
            "1563/1563 [==============================] - 20s 13ms/step - loss: 1.1310 - accuracy: 0.5986\n",
            "Epoch 42/50\n",
            "1563/1563 [==============================] - 20s 13ms/step - loss: 1.1229 - accuracy: 0.6040\n",
            "Epoch 43/50\n",
            "1563/1563 [==============================] - 20s 13ms/step - loss: 1.1157 - accuracy: 0.6048\n",
            "Epoch 44/50\n",
            "1563/1563 [==============================] - 20s 13ms/step - loss: 1.1025 - accuracy: 0.6083\n",
            "Epoch 45/50\n",
            "1563/1563 [==============================] - 20s 13ms/step - loss: 1.0943 - accuracy: 0.6127\n",
            "Epoch 46/50\n",
            "1563/1563 [==============================] - 20s 12ms/step - loss: 1.0847 - accuracy: 0.6172\n",
            "Epoch 47/50\n",
            "1563/1563 [==============================] - 20s 13ms/step - loss: 1.0766 - accuracy: 0.6176\n",
            "Epoch 48/50\n",
            "1563/1563 [==============================] - 20s 13ms/step - loss: 1.0771 - accuracy: 0.6166\n",
            "Epoch 49/50\n",
            "1563/1563 [==============================] - 20s 13ms/step - loss: 1.0589 - accuracy: 0.6243\n",
            "Epoch 50/50\n",
            "1563/1563 [==============================] - 20s 13ms/step - loss: 1.0591 - accuracy: 0.6244\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fd79655b3d0>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_nn_2.evaluate(X_test,y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P7vQNrHzmgLt",
        "outputId": "64ce773f-87c8-4d46-c151-cac1a3727bb7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 2s 6ms/step - loss: 1.2531 - accuracy: 0.5702\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.2530758380889893, 0.5702000260353088]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Model 3:"
      ],
      "metadata": {
        "id": "9HhuX2kYm-iE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_nn_3= models.Sequential([\n",
        "        layers.Flatten(input_shape=(32,32,3)),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        layers.Dense(3072, activation='relu'),\n",
        "        tf.keras.layers.Dropout(0.5),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        layers.Dense(1024, activation='relu'),\n",
        "        tf.keras.layers.Dropout(0.5),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        layers.Dense(10, activation='sigmoid'),\n",
        "        #tf.keras.layers.Dropout(0.5)  \n",
        "    ])\n",
        "\n",
        "model_nn_3.compile(optimizer='SGD',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "history=model_nn_3.fit(X_train, y_train, epochs=50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xCWAvjMpm9Fw",
        "outputId": "8c8346f2-7fb8-4717-cced-423147fd4a02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "1563/1563 [==============================] - 14s 9ms/step - loss: 2.0202 - accuracy: 0.3172\n",
            "Epoch 2/50\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 1.7343 - accuracy: 0.3830\n",
            "Epoch 3/50\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 1.6615 - accuracy: 0.4065\n",
            "Epoch 4/50\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 1.6198 - accuracy: 0.4214\n",
            "Epoch 5/50\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 1.5842 - accuracy: 0.4373\n",
            "Epoch 6/50\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 1.5578 - accuracy: 0.4448\n",
            "Epoch 7/50\n",
            "1563/1563 [==============================] - 13s 9ms/step - loss: 1.5336 - accuracy: 0.4556\n",
            "Epoch 8/50\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 1.5137 - accuracy: 0.4628\n",
            "Epoch 9/50\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 1.4942 - accuracy: 0.4671\n",
            "Epoch 10/50\n",
            "1563/1563 [==============================] - 14s 9ms/step - loss: 1.4783 - accuracy: 0.4729\n",
            "Epoch 11/50\n",
            "1563/1563 [==============================] - 13s 9ms/step - loss: 1.4541 - accuracy: 0.4814\n",
            "Epoch 12/50\n",
            "1563/1563 [==============================] - 13s 9ms/step - loss: 1.4483 - accuracy: 0.4825\n",
            "Epoch 13/50\n",
            "1563/1563 [==============================] - 15s 10ms/step - loss: 1.4319 - accuracy: 0.4890\n",
            "Epoch 14/50\n",
            "1563/1563 [==============================] - 13s 9ms/step - loss: 1.4217 - accuracy: 0.4937\n",
            "Epoch 15/50\n",
            "1563/1563 [==============================] - 13s 9ms/step - loss: 1.4082 - accuracy: 0.4982\n",
            "Epoch 16/50\n",
            "1563/1563 [==============================] - 13s 9ms/step - loss: 1.3909 - accuracy: 0.5053\n",
            "Epoch 17/50\n",
            "1563/1563 [==============================] - 14s 9ms/step - loss: 1.3802 - accuracy: 0.5097\n",
            "Epoch 18/50\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 1.3655 - accuracy: 0.5120\n",
            "Epoch 19/50\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 1.3519 - accuracy: 0.5183\n",
            "Epoch 20/50\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 1.3426 - accuracy: 0.5207\n",
            "Epoch 21/50\n",
            "1563/1563 [==============================] - 14s 9ms/step - loss: 1.3295 - accuracy: 0.5257\n",
            "Epoch 22/50\n",
            "1563/1563 [==============================] - 14s 9ms/step - loss: 1.3275 - accuracy: 0.5276\n",
            "Epoch 23/50\n",
            "1563/1563 [==============================] - 13s 9ms/step - loss: 1.3147 - accuracy: 0.5311\n",
            "Epoch 24/50\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 1.2998 - accuracy: 0.5389\n",
            "Epoch 25/50\n",
            "1563/1563 [==============================] - 14s 9ms/step - loss: 1.2921 - accuracy: 0.5373\n",
            "Epoch 26/50\n",
            "1563/1563 [==============================] - 14s 9ms/step - loss: 1.2833 - accuracy: 0.5410\n",
            "Epoch 27/50\n",
            "1563/1563 [==============================] - 14s 9ms/step - loss: 1.2744 - accuracy: 0.5460\n",
            "Epoch 28/50\n",
            "1563/1563 [==============================] - 14s 9ms/step - loss: 1.2613 - accuracy: 0.5488\n",
            "Epoch 29/50\n",
            "1563/1563 [==============================] - 13s 9ms/step - loss: 1.2543 - accuracy: 0.5539\n",
            "Epoch 30/50\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 1.2457 - accuracy: 0.5561\n",
            "Epoch 31/50\n",
            "1563/1563 [==============================] - 13s 9ms/step - loss: 1.2377 - accuracy: 0.5576\n",
            "Epoch 32/50\n",
            "1563/1563 [==============================] - 13s 9ms/step - loss: 1.2324 - accuracy: 0.5591\n",
            "Epoch 33/50\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 1.2216 - accuracy: 0.5648\n",
            "Epoch 34/50\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 1.2086 - accuracy: 0.5690\n",
            "Epoch 35/50\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 1.2059 - accuracy: 0.5678\n",
            "Epoch 36/50\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 1.1971 - accuracy: 0.5775\n",
            "Epoch 37/50\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 1.1854 - accuracy: 0.5778\n",
            "Epoch 38/50\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 1.1799 - accuracy: 0.5781\n",
            "Epoch 39/50\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 1.1697 - accuracy: 0.5837\n",
            "Epoch 40/50\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 1.1623 - accuracy: 0.5841\n",
            "Epoch 41/50\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 1.1504 - accuracy: 0.5877\n",
            "Epoch 42/50\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 1.1489 - accuracy: 0.5905\n",
            "Epoch 43/50\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 1.1402 - accuracy: 0.5932\n",
            "Epoch 44/50\n",
            "1563/1563 [==============================] - 14s 9ms/step - loss: 1.1322 - accuracy: 0.5949\n",
            "Epoch 45/50\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 1.1223 - accuracy: 0.5956\n",
            "Epoch 46/50\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 1.1156 - accuracy: 0.6000\n",
            "Epoch 47/50\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 1.1098 - accuracy: 0.6028\n",
            "Epoch 48/50\n",
            "1563/1563 [==============================] - 13s 9ms/step - loss: 1.1009 - accuracy: 0.6082\n",
            "Epoch 49/50\n",
            "1563/1563 [==============================] - 13s 9ms/step - loss: 1.0947 - accuracy: 0.6081\n",
            "Epoch 50/50\n",
            "1563/1563 [==============================] - 13s 9ms/step - loss: 1.0913 - accuracy: 0.6093\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_nn_3.evaluate(X_test,y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L5oVL6a4WRmr",
        "outputId": "e656f625-d414-4ae4-e367-33765983490c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 2s 5ms/step - loss: 1.2023 - accuracy: 0.5767\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.202298641204834, 0.57669997215271]"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"The first model :\"+format(model_nn_1.evaluate(X_test,y_test)))\n",
        "print(\"The second model :\"+format(model_nn_2.evaluate(X_test,y_test)))\n",
        "print(\"The third model :\"+format(model_nn_3.evaluate(X_test,y_test)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9qi1gCeu62zV",
        "outputId": "682d2e84-6748-43f7-8167-6c06a73d7bcc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 2s 5ms/step - loss: 1.2050 - accuracy: 0.5755\n",
            "The first model :[1.2050020694732666, 0.5755000114440918]\n",
            "313/313 [==============================] - 2s 7ms/step - loss: 1.2531 - accuracy: 0.5702\n",
            "The second model :[1.2530758380889893, 0.5702000260353088]\n",
            "313/313 [==============================] - 2s 5ms/step - loss: 1.2023 - accuracy: 0.5767\n",
            "The third model :[1.202298641204834, 0.57669997215271]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Comparing between the models :***\n",
        "- We tried to change the activations fcts, the best result was with two \"relu\" and a \"sigmoid\" fonction.\n",
        "\n",
        "- Changing the number of neurons dosn't really help, it' making the model overfit a little!\n",
        "\n",
        "- Changing the structure of the neural network ('reducing the complexity doesn't make it better in this case.\n",
        "\n",
        "- The accuracy of the three models is almost the same, we couldn't reach a better result. "
      ],
      "metadata": {
        "id": "FcvOTCr4iS6V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "91xOuh1eihTA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#loss of the model_nn_3\n",
        "plt.plot(history.history['loss'])\n",
        "#plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train'], loc='upper left')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "v2Wg4kFPoFB_",
        "outputId": "96f24b7e-e4f8-417a-907a-9c1261a49fed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fc8lCKMlGMrb"
      },
      "source": [
        "## Few more experiments to go\n",
        "\n",
        "1.   **Exploring more regularization techniques:** Try at least 2 regularization techniques separately and combined.  A non-exhaustive list of regularization techniques you can test\n",
        "\n",
        "\n",
        "> * L1 and L2 regularization\n",
        "* Early stopping\n",
        "* Data augmentation\n",
        "* Decreasing the complexity of the model\n",
        "\n",
        "2.   **Hyperparameters' tuning:** Try to tune the learning parameters using the tuning strategies we learned about:\n",
        "\n",
        "> * Learning rate\n",
        "* Mini-batch size\n",
        "* The optimizer and its parameters\n",
        "\n",
        "3. Analyze the impact of each of the applied techniques. What were the most effective ones? What were the hypeparameters that affects the results the most?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OBXIjaAVGMPd"
      },
      "outputs": [],
      "source": [
        "# Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " - Regularization techniques:"
      ],
      "metadata": {
        "id": "-ntHgc1Hov-I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- L1 & L2 :"
      ],
      "metadata": {
        "id": "4vwzhW3K2lT_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " model_l2= models.Sequential([\n",
        "        layers.Flatten(input_shape=(32,32,3)),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        layers.Dense(3000, activation='LeakyReLU',kernel_regularizer='l2'),\n",
        "        #tf.keras.layers.Dropout(0.5),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        layers.Dense(1000, activation='LeakyReLU'),\n",
        "        #tf.keras.layers.Dropout(0.5),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        layers.Dense(10, activation='sigmoid',kernel_regularizer='l2'),\n",
        "        \n",
        "    ])\n",
        "\n",
        " model_l2.compile(optimizer='SGD',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        " model_l2.fit(X_train, y_train, epochs=50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_9AomZPwpHZk",
        "outputId": "71e36d3f-d676-46fb-c990-1a9cf5dbf542"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "1563/1563 [==============================] - 19s 12ms/step - loss: 24.4821 - accuracy: 0.3965\n",
            "Epoch 2/50\n",
            "1563/1563 [==============================] - 18s 12ms/step - loss: 13.7268 - accuracy: 0.4667\n",
            "Epoch 3/50\n",
            "1563/1563 [==============================] - 18s 11ms/step - loss: 8.0609 - accuracy: 0.4844\n",
            "Epoch 4/50\n",
            "1563/1563 [==============================] - 18s 11ms/step - loss: 5.0374 - accuracy: 0.4998\n",
            "Epoch 5/50\n",
            "1563/1563 [==============================] - 18s 11ms/step - loss: 3.4276 - accuracy: 0.5024\n",
            "Epoch 6/50\n",
            "1563/1563 [==============================] - 18s 11ms/step - loss: 2.5598 - accuracy: 0.5135\n",
            "Epoch 7/50\n",
            "1563/1563 [==============================] - 18s 11ms/step - loss: 2.0933 - accuracy: 0.5165\n",
            "Epoch 8/50\n",
            "1563/1563 [==============================] - 18s 11ms/step - loss: 1.8371 - accuracy: 0.5207\n",
            "Epoch 9/50\n",
            "1563/1563 [==============================] - 18s 11ms/step - loss: 1.7014 - accuracy: 0.5263\n",
            "Epoch 10/50\n",
            "1563/1563 [==============================] - 18s 11ms/step - loss: 1.6221 - accuracy: 0.5298\n",
            "Epoch 11/50\n",
            "1563/1563 [==============================] - 17s 11ms/step - loss: 1.5755 - accuracy: 0.5368\n",
            "Epoch 12/50\n",
            "1563/1563 [==============================] - 18s 11ms/step - loss: 1.5478 - accuracy: 0.5360\n",
            "Epoch 13/50\n",
            "1563/1563 [==============================] - 17s 11ms/step - loss: 1.5372 - accuracy: 0.5382\n",
            "Epoch 14/50\n",
            "1563/1563 [==============================] - 18s 11ms/step - loss: 1.5283 - accuracy: 0.5412\n",
            "Epoch 15/50\n",
            "1563/1563 [==============================] - 17s 11ms/step - loss: 1.5120 - accuracy: 0.5486\n",
            "Epoch 16/50\n",
            "1563/1563 [==============================] - 18s 11ms/step - loss: 1.5077 - accuracy: 0.5472\n",
            "Epoch 17/50\n",
            "1563/1563 [==============================] - 18s 11ms/step - loss: 1.5066 - accuracy: 0.5476\n",
            "Epoch 18/50\n",
            "1563/1563 [==============================] - 18s 11ms/step - loss: 1.5001 - accuracy: 0.5529\n",
            "Epoch 19/50\n",
            "1563/1563 [==============================] - 18s 11ms/step - loss: 1.4968 - accuracy: 0.5544\n",
            "Epoch 20/50\n",
            "1563/1563 [==============================] - 18s 11ms/step - loss: 1.4924 - accuracy: 0.5570\n",
            "Epoch 21/50\n",
            "1563/1563 [==============================] - 17s 11ms/step - loss: 1.4881 - accuracy: 0.5582\n",
            "Epoch 22/50\n",
            "1563/1563 [==============================] - 18s 11ms/step - loss: 1.4893 - accuracy: 0.5609\n",
            "Epoch 23/50\n",
            "1563/1563 [==============================] - 18s 11ms/step - loss: 1.4825 - accuracy: 0.5627\n",
            "Epoch 24/50\n",
            "1563/1563 [==============================] - 17s 11ms/step - loss: 1.4784 - accuracy: 0.5667\n",
            "Epoch 25/50\n",
            "1563/1563 [==============================] - 17s 11ms/step - loss: 1.4838 - accuracy: 0.5618\n",
            "Epoch 26/50\n",
            "1563/1563 [==============================] - 17s 11ms/step - loss: 1.4680 - accuracy: 0.5682\n",
            "Epoch 27/50\n",
            "1563/1563 [==============================] - 17s 11ms/step - loss: 1.4716 - accuracy: 0.5680\n",
            "Epoch 28/50\n",
            "1563/1563 [==============================] - 17s 11ms/step - loss: 1.4683 - accuracy: 0.5719\n",
            "Epoch 29/50\n",
            "1563/1563 [==============================] - 17s 11ms/step - loss: 1.4670 - accuracy: 0.5704\n",
            "Epoch 30/50\n",
            "1563/1563 [==============================] - 18s 11ms/step - loss: 1.4671 - accuracy: 0.5705\n",
            "Epoch 31/50\n",
            "1563/1563 [==============================] - 18s 11ms/step - loss: 1.4636 - accuracy: 0.5752\n",
            "Epoch 32/50\n",
            "1563/1563 [==============================] - 18s 11ms/step - loss: 1.4612 - accuracy: 0.5740\n",
            "Epoch 33/50\n",
            "1563/1563 [==============================] - 17s 11ms/step - loss: 1.4595 - accuracy: 0.5770\n",
            "Epoch 34/50\n",
            "1563/1563 [==============================] - 17s 11ms/step - loss: 1.4533 - accuracy: 0.5769\n",
            "Epoch 35/50\n",
            "1563/1563 [==============================] - 17s 11ms/step - loss: 1.4558 - accuracy: 0.5779\n",
            "Epoch 36/50\n",
            "1563/1563 [==============================] - 17s 11ms/step - loss: 1.4536 - accuracy: 0.5807\n",
            "Epoch 37/50\n",
            "1563/1563 [==============================] - 17s 11ms/step - loss: 1.4521 - accuracy: 0.5801\n",
            "Epoch 38/50\n",
            "1563/1563 [==============================] - 17s 11ms/step - loss: 1.4481 - accuracy: 0.5825\n",
            "Epoch 39/50\n",
            "1563/1563 [==============================] - 17s 11ms/step - loss: 1.4493 - accuracy: 0.5807\n",
            "Epoch 40/50\n",
            "1563/1563 [==============================] - 17s 11ms/step - loss: 1.4449 - accuracy: 0.5834\n",
            "Epoch 41/50\n",
            "1563/1563 [==============================] - 17s 11ms/step - loss: 1.4427 - accuracy: 0.5854\n",
            "Epoch 42/50\n",
            "1563/1563 [==============================] - 17s 11ms/step - loss: 1.4434 - accuracy: 0.5835\n",
            "Epoch 43/50\n",
            "1563/1563 [==============================] - 17s 11ms/step - loss: 1.4442 - accuracy: 0.5832\n",
            "Epoch 44/50\n",
            "1563/1563 [==============================] - 17s 11ms/step - loss: 1.4372 - accuracy: 0.5893\n",
            "Epoch 45/50\n",
            "1563/1563 [==============================] - 17s 11ms/step - loss: 1.4394 - accuracy: 0.5885\n",
            "Epoch 46/50\n",
            "1563/1563 [==============================] - 18s 11ms/step - loss: 1.4335 - accuracy: 0.5898\n",
            "Epoch 47/50\n",
            "1563/1563 [==============================] - 18s 11ms/step - loss: 1.4357 - accuracy: 0.5923\n",
            "Epoch 48/50\n",
            "1563/1563 [==============================] - 18s 11ms/step - loss: 1.4321 - accuracy: 0.5951\n",
            "Epoch 49/50\n",
            "1563/1563 [==============================] - 17s 11ms/step - loss: 1.4313 - accuracy: 0.5901\n",
            "Epoch 50/50\n",
            "1563/1563 [==============================] - 17s 11ms/step - loss: 1.4289 - accuracy: 0.5934\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7ff64004a710>"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- early stopping:"
      ],
      "metadata": {
        "id": "IimAV6AI2vcg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#early stopping:\n",
        "model_erl= models.Sequential([\n",
        "        layers.Flatten(input_shape=(32,32,3)),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        layers.Dense(3072, activation='relu'),\n",
        "        tf.keras.layers.Dropout(0.5),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        layers.Dense(1024, activation='relu'),\n",
        "        tf.keras.layers.Dropout(0.5),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        layers.Dense(10, activation='sigmoid'),\n",
        "        #tf.keras.layers.Dropout(0.5)  \n",
        "    ])\n",
        "\n",
        "model_erl.compile(optimizer='SGD',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)\n",
        "\n",
        "history_erl=model_erl.fit(X_train, y_train, epochs=50,callbacks=[callback] )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aQ7WJCN22AkI",
        "outputId": "1fe0d713-7427-49b9-8bdc-b6421581ac88"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "1563/1563 [==============================] - 14s 9ms/step - loss: 2.0274 - accuracy: 0.3145\n",
            "Epoch 2/50\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 1.7355 - accuracy: 0.3853\n",
            "Epoch 3/50\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 1.6629 - accuracy: 0.4086\n",
            "Epoch 4/50\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 1.6254 - accuracy: 0.4203\n",
            "Epoch 5/50\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 1.5884 - accuracy: 0.4330\n",
            "Epoch 6/50\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 1.5598 - accuracy: 0.4416\n",
            "Epoch 7/50\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 1.5341 - accuracy: 0.4535\n",
            "Epoch 8/50\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 1.5123 - accuracy: 0.4618\n",
            "Epoch 9/50\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 1.4952 - accuracy: 0.4638\n",
            "Epoch 10/50\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 1.4721 - accuracy: 0.4764\n",
            "Epoch 11/50\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 1.4562 - accuracy: 0.4815\n",
            "Epoch 12/50\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 1.4437 - accuracy: 0.4859\n",
            "Epoch 13/50\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 1.4294 - accuracy: 0.4890\n",
            "Epoch 14/50\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 1.4189 - accuracy: 0.4928\n",
            "Epoch 15/50\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 1.3962 - accuracy: 0.5018\n",
            "Epoch 16/50\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 1.3943 - accuracy: 0.5027\n",
            "Epoch 17/50\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 1.3761 - accuracy: 0.5083\n",
            "Epoch 18/50\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 1.3689 - accuracy: 0.5140\n",
            "Epoch 19/50\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 1.3530 - accuracy: 0.5165\n",
            "Epoch 20/50\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 1.3452 - accuracy: 0.5208\n",
            "Epoch 21/50\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 1.3316 - accuracy: 0.5228\n",
            "Epoch 22/50\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 1.3196 - accuracy: 0.5283\n",
            "Epoch 23/50\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 1.3129 - accuracy: 0.5312\n",
            "Epoch 24/50\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 1.2999 - accuracy: 0.5353\n",
            "Epoch 25/50\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 1.2916 - accuracy: 0.5389\n",
            "Epoch 26/50\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 1.2784 - accuracy: 0.5438\n",
            "Epoch 27/50\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 1.2711 - accuracy: 0.5445\n",
            "Epoch 28/50\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 1.2662 - accuracy: 0.5473\n",
            "Epoch 29/50\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 1.2496 - accuracy: 0.5537\n",
            "Epoch 30/50\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 1.2443 - accuracy: 0.5561\n",
            "Epoch 31/50\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 1.2309 - accuracy: 0.5610\n",
            "Epoch 32/50\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 1.2346 - accuracy: 0.5608\n",
            "Epoch 33/50\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 1.2177 - accuracy: 0.5673\n",
            "Epoch 34/50\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 1.2149 - accuracy: 0.5643\n",
            "Epoch 35/50\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 1.2040 - accuracy: 0.5698\n",
            "Epoch 36/50\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 1.1894 - accuracy: 0.5755\n",
            "Epoch 37/50\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 1.1870 - accuracy: 0.5769\n",
            "Epoch 38/50\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 1.1767 - accuracy: 0.5807\n",
            "Epoch 39/50\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 1.1691 - accuracy: 0.5819\n",
            "Epoch 40/50\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 1.1647 - accuracy: 0.5852\n",
            "Epoch 41/50\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 1.1519 - accuracy: 0.5879\n",
            "Epoch 42/50\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 1.1519 - accuracy: 0.5888\n",
            "Epoch 43/50\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 1.1392 - accuracy: 0.5940\n",
            "Epoch 44/50\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 1.1331 - accuracy: 0.5938\n",
            "Epoch 45/50\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 1.1249 - accuracy: 0.5971\n",
            "Epoch 46/50\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 1.1168 - accuracy: 0.6022\n",
            "Epoch 47/50\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 1.1068 - accuracy: 0.6052\n",
            "Epoch 48/50\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 1.1074 - accuracy: 0.6064\n",
            "Epoch 49/50\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 1.0994 - accuracy: 0.6084\n",
            "Epoch 50/50\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 1.0878 - accuracy: 0.6125\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Hyperparameters' tuning:\n"
      ],
      "metadata": {
        "id": "hYhUQJOzEhsd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " model_hyp= models.Sequential([\n",
        "        layers.Flatten(input_shape=(32,32,3)),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        layers.Dense(3000, activation='LeakyReLU',kernel_regularizer='l2'),\n",
        "        #tf.keras.layers.Dropout(0.5),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        layers.Dense(1000, activation='LeakyReLU'),\n",
        "        #tf.keras.layers.Dropout(0.5),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        layers.Dense(10, activation='sigmoid',kernel_regularizer='l2'),\n",
        "        \n",
        "    ])\n",
        " lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
        "    initial_learning_rate=0.01,\n",
        "    decay_steps=10000,\n",
        "    decay_rate=0.9)\n",
        "optimizer_1 = keras.optimizers.SGD(learning_rate=lr_schedule)\n",
        "\n",
        " model_hyp.compile(optimizer=optimizer_1,\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        " model_hyp.fit(X_train, y_train, epochs=50)"
      ],
      "metadata": {
        "id": "iejwJNu_EpA0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81daeb8d-73a3-42d4-b647-0da2258c3caf"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "1563/1563 [==============================] - 19s 11ms/step - loss: 24.5229 - accuracy: 0.4003\n",
            "Epoch 2/50\n",
            "1563/1563 [==============================] - 18s 11ms/step - loss: 13.8674 - accuracy: 0.4660\n",
            "Epoch 3/50\n",
            "1563/1563 [==============================] - 18s 12ms/step - loss: 8.2624 - accuracy: 0.4870\n",
            "Epoch 4/50\n",
            "1563/1563 [==============================] - 17s 11ms/step - loss: 5.2457 - accuracy: 0.4995\n",
            "Epoch 5/50\n",
            "1563/1563 [==============================] - 17s 11ms/step - loss: 3.6034 - accuracy: 0.5111\n",
            "Epoch 6/50\n",
            "1563/1563 [==============================] - 17s 11ms/step - loss: 2.7000 - accuracy: 0.5171\n",
            "Epoch 7/50\n",
            "1563/1563 [==============================] - 17s 11ms/step - loss: 2.1947 - accuracy: 0.5202\n",
            "Epoch 8/50\n",
            "1563/1563 [==============================] - 17s 11ms/step - loss: 1.9023 - accuracy: 0.5296\n",
            "Epoch 9/50\n",
            "1563/1563 [==============================] - 17s 11ms/step - loss: 1.7348 - accuracy: 0.5348\n",
            "Epoch 10/50\n",
            "1563/1563 [==============================] - 17s 11ms/step - loss: 1.6395 - accuracy: 0.5373\n",
            "Epoch 11/50\n",
            "1563/1563 [==============================] - 17s 11ms/step - loss: 1.5738 - accuracy: 0.5443\n",
            "Epoch 12/50\n",
            "1563/1563 [==============================] - 17s 11ms/step - loss: 1.5368 - accuracy: 0.5482\n",
            "Epoch 13/50\n",
            "1563/1563 [==============================] - 17s 11ms/step - loss: 1.5101 - accuracy: 0.5524\n",
            "Epoch 14/50\n",
            "1563/1563 [==============================] - 17s 11ms/step - loss: 1.4902 - accuracy: 0.5578\n",
            "Epoch 15/50\n",
            "1563/1563 [==============================] - 17s 11ms/step - loss: 1.4730 - accuracy: 0.5664\n",
            "Epoch 16/50\n",
            "1563/1563 [==============================] - 17s 11ms/step - loss: 1.4586 - accuracy: 0.5671\n",
            "Epoch 17/50\n",
            "1563/1563 [==============================] - 17s 11ms/step - loss: 1.4537 - accuracy: 0.5712\n",
            "Epoch 18/50\n",
            "1563/1563 [==============================] - 17s 11ms/step - loss: 1.4425 - accuracy: 0.5744\n",
            "Epoch 19/50\n",
            "1563/1563 [==============================] - 17s 11ms/step - loss: 1.4360 - accuracy: 0.5798\n",
            "Epoch 20/50\n",
            "1563/1563 [==============================] - 17s 11ms/step - loss: 1.4291 - accuracy: 0.5816\n",
            "Epoch 21/50\n",
            "1563/1563 [==============================] - 17s 11ms/step - loss: 1.4216 - accuracy: 0.5886\n",
            "Epoch 22/50\n",
            "1563/1563 [==============================] - 17s 11ms/step - loss: 1.4101 - accuracy: 0.5921\n",
            "Epoch 23/50\n",
            "1563/1563 [==============================] - 17s 11ms/step - loss: 1.4046 - accuracy: 0.5973\n",
            "Epoch 24/50\n",
            "1563/1563 [==============================] - 17s 11ms/step - loss: 1.3984 - accuracy: 0.5995\n",
            "Epoch 25/50\n",
            "1563/1563 [==============================] - 17s 11ms/step - loss: 1.3920 - accuracy: 0.6017\n",
            "Epoch 26/50\n",
            "1563/1563 [==============================] - 17s 11ms/step - loss: 1.3865 - accuracy: 0.6045\n",
            "Epoch 27/50\n",
            "1563/1563 [==============================] - 17s 11ms/step - loss: 1.3824 - accuracy: 0.6069\n",
            "Epoch 28/50\n",
            "1563/1563 [==============================] - 17s 11ms/step - loss: 1.3724 - accuracy: 0.6121\n",
            "Epoch 29/50\n",
            "1563/1563 [==============================] - 17s 11ms/step - loss: 1.3662 - accuracy: 0.6164\n",
            "Epoch 30/50\n",
            "1563/1563 [==============================] - 17s 11ms/step - loss: 1.3666 - accuracy: 0.6164\n",
            "Epoch 31/50\n",
            "1563/1563 [==============================] - 17s 11ms/step - loss: 1.3603 - accuracy: 0.6221\n",
            "Epoch 32/50\n",
            "1563/1563 [==============================] - 17s 11ms/step - loss: 1.3496 - accuracy: 0.6241\n",
            "Epoch 33/50\n",
            "1563/1563 [==============================] - 17s 11ms/step - loss: 1.3466 - accuracy: 0.6279\n",
            "Epoch 34/50\n",
            "1563/1563 [==============================] - 17s 11ms/step - loss: 1.3386 - accuracy: 0.6290\n",
            "Epoch 35/50\n",
            "1563/1563 [==============================] - 17s 11ms/step - loss: 1.3349 - accuracy: 0.6342\n",
            "Epoch 36/50\n",
            "1563/1563 [==============================] - 18s 11ms/step - loss: 1.3271 - accuracy: 0.6364\n",
            "Epoch 37/50\n",
            "1563/1563 [==============================] - 17s 11ms/step - loss: 1.3210 - accuracy: 0.6404\n",
            "Epoch 38/50\n",
            "1563/1563 [==============================] - 17s 11ms/step - loss: 1.3161 - accuracy: 0.6408\n",
            "Epoch 39/50\n",
            "1563/1563 [==============================] - 17s 11ms/step - loss: 1.3088 - accuracy: 0.6459\n",
            "Epoch 40/50\n",
            "1563/1563 [==============================] - 17s 11ms/step - loss: 1.3019 - accuracy: 0.6491\n",
            "Epoch 41/50\n",
            "1563/1563 [==============================] - 17s 11ms/step - loss: 1.2974 - accuracy: 0.6519\n",
            "Epoch 42/50\n",
            "1563/1563 [==============================] - 17s 11ms/step - loss: 1.2915 - accuracy: 0.6571\n",
            "Epoch 43/50\n",
            "1563/1563 [==============================] - 17s 11ms/step - loss: 1.2855 - accuracy: 0.6568\n",
            "Epoch 44/50\n",
            "1563/1563 [==============================] - 17s 11ms/step - loss: 1.2801 - accuracy: 0.6623\n",
            "Epoch 45/50\n",
            "1563/1563 [==============================] - 17s 11ms/step - loss: 1.2747 - accuracy: 0.6654\n",
            "Epoch 46/50\n",
            "1563/1563 [==============================] - 17s 11ms/step - loss: 1.2667 - accuracy: 0.6684\n",
            "Epoch 47/50\n",
            "1563/1563 [==============================] - 18s 11ms/step - loss: 1.2627 - accuracy: 0.6718\n",
            "Epoch 48/50\n",
            "1563/1563 [==============================] - 21s 13ms/step - loss: 1.2579 - accuracy: 0.6751\n",
            "Epoch 49/50\n",
            "1563/1563 [==============================] - 18s 12ms/step - loss: 1.2532 - accuracy: 0.6745\n",
            "Epoch 50/50\n",
            "1563/1563 [==============================] - 19s 12ms/step - loss: 1.2432 - accuracy: 0.6814\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7ff5b9cb5710>"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Evaluating the models:\n",
        "model_l2.evaluate(X_test,y_test)\n",
        "model_erl.evaluate(X_test,y_test)\n",
        "model_hyp.evaluate(X_test,y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qH_hcqtldaF6",
        "outputId": "1770f339-a167-4069-ae99-40392c8e8081"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 2s 6ms/step - loss: 1.5948 - accuracy: 0.5344\n",
            "313/313 [==============================] - 2s 5ms/step - loss: 1.2062 - accuracy: 0.5760\n",
            "313/313 [==============================] - 2s 6ms/step - loss: 1.5952 - accuracy: 0.5482\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.5951582193374634, 0.5482000112533569]"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Analyse:***\n",
        "- The most effective technique was the \"early stopping\" methode, but there was no overfitting.\n",
        "- The \"Learning rate\" and \" decay rate\" were the hyperparameters that effects the most on the result in the train and perform bad in the test, it's just overfiting again!"
      ],
      "metadata": {
        "id": "5Y5JnDWCtgHS"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qcFRgyd_TAOx"
      },
      "source": [
        "## Answer the following questions\n",
        "*Hint: Do your own research to answer these questions, none of the questions is answered in the previous lessons*\n",
        "\n",
        "\n",
        "1.   Why we can’t reach a good accuracy on this task?\n",
        "2.   Explain why fully connected neural networks are Not efficient on image tasks\n",
        "3.   What architecture can be used  for such tasks? Why they are more adapted for that?\n",
        "\n",
        "\\"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- We can't reach a good accuracy on this task because it's an image classification \n",
        "task, and fully connected neural perform bad at this\n",
        "\n",
        "- the image has a large input, in this case we had 32x32x3, the model will calculated 3072 weight and bias, more complexity and without forgetting the high chance of overfitting\n",
        "\n",
        "- CNNs(Convolution neural network) works better than fully connected networks for image processing, it will convert the large input to a lower one and this could be useful especially for image that had large number of pixels "
      ],
      "metadata": {
        "id": "iG-Qsd0upAfQ"
      }
    }
  ],
  "metadata": {
    "colab": {
      "name": "_1Copie de Unit 4 Project - Questions.ipynb",
      "provenance": [],
      "toc_visible": true,
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}